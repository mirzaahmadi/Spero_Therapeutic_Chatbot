{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script loads therapy conversation data from a CSV file, splits it into chunks of dialogue turns, embeds the chunks using OpenAI,\n",
        "and stores them in a Chroma_DB database. This file was ran on Google Collab because the memory requirements were too great for my local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Fo6zYfh950",
        "outputId": "19b4ac6a-1178-480d-d4f8-994034a5fdb6"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv langchain-core langchain-openai langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAdnhEkCe7oF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import ast\n",
        "from uuid import uuid4\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "import sys\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0sJgo9LfIfT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# === LOAD API KEY ===\n",
        "load_dotenv() # This loads the API key from OpenAI (in the .env file) allowing us to use their model\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "DATA_PATH = r\"data/preprocessed_dataset.csv\"  # Path to your dataset\n",
        "CHROMA_PATH = r\"chroma_db\"  # Directory where Chroma saves (persists) its database files\n",
        "SIZE_OF_CHUNK = 4  # Number of utterances per chunk\n",
        "BATCH_SIZE = 1000\n",
        "SLEEP_SECONDS = 2\n",
        "\n",
        "# === TESTING IF CHROMA DATABASE WORK ===\n",
        "os.makedirs(CHROMA_PATH, exist_ok=True)\n",
        "print(f\"üíæ Using Chroma persist directory: {CHROMA_PATH}\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# === INIT EMBEDDINGS ===\n",
        "embedder = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "print(\"üß™ Testing embedding...\")\n",
        "sys.stdout.flush()\n",
        "test_vec = embedder.embed_query(\"THERAPIST: How are you feeling today?\")\n",
        "print(\"‚úÖ Embedding worked. Length:\", len(test_vec))\n",
        "sys.stdout.flush()\n",
        "\n",
        "# === LOAD AND PARSE THE 'conversations' COLUMN ===\n",
        "\"\"\" \n",
        "This takes in each row of the dataset, returning a list of 'all_turns' so I have a list of every single utterrance, from client and therapist alike\n",
        "\"\"\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "def parse_conversations(row, idx=None):\n",
        "    try:\n",
        "        return ast.literal_eval(row) # This line takes the words in each row and returns them as a string object\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to parse row {idx}: {e}\")\n",
        "        return []\n",
        "\n",
        "all_turns = []\n",
        "# Loop through every row, obtain the utterrances as a str object, store each utterrance in a all_turns list\n",
        "for idx, row in df['conversations'].items():\n",
        "    turns = parse_conversations(row, idx)\n",
        "    all_turns.extend(turns) # all_turns now holds every single utterrance (in order)\n",
        "\n",
        "print(f\"\\n‚úÖ Parsed {len(all_turns)} total dialogue turns.\")\n",
        "sys.stdout.flush() #This forces the 'print' output to appear immediately\n",
        "\n",
        "# === CHUNK ===\n",
        "\"\"\" \n",
        "Each 'chunk' will be 4 utterrances (which we specified), the 'chunks_text' will return all the chunks, with 4 utterrances in each.\n",
        "\"\"\"\n",
        "def dialogue_chunker(dialogues, max_turns=SIZE_OF_CHUNK):\n",
        "    # This loops through all the utterances in all_turns, adding 4 at a time to a chunk\n",
        "    chunks = []\n",
        "    for i in range(0, len(dialogues), max_turns):\n",
        "        chunk = dialogues[i:i + max_turns]\n",
        "        \"\"\" \n",
        "        Assume that each chunk looks like: \n",
        "        chunk = [\n",
        "            {'role': 'Therapist', 'text': 'How are you feeling today?'},\n",
        "            {'role': 'Client', 'text': 'I‚Äôm not doing great.'},\n",
        "            {'role': 'Therapist', 'text': 'Can you tell me more about that?'},\n",
        "            {'role': 'Client', 'text': 'I‚Äôve been really anxious lately.'}\n",
        "            ]\n",
        "        This list comprehension line below loops through every entry within 'chunk' and only outputs the values for 'role' and 'text' so that the chunks list is clean. Also, each of the four entries within chunk_text is held together by new line characters, so in the end, it looks like this:\n",
        "        chunk_texts = [\n",
        "        \"Therapist: How are you feeling today?\\nClient: I‚Äôm not doing great.\\nTherapist: Can you tell me more about that?\\nClient: I‚Äôve been really anxious lately.\",\n",
        "        \n",
        "        \"Therapist: What do you think is triggering your anxiety?\\nClient: I‚Äôm not really sure.\\nTherapist: When did it start?\\nClient: A few weeks ago.\",\n",
        "        \n",
        "        ...\n",
        "        ]\n",
        "        \"\"\"\n",
        "        chunk_text = \"\\n\".join([f\"{d['role']}: {d['text']}\" for d in chunk])\n",
        "        chunks.append(chunk_text)\n",
        "    return chunks\n",
        "\n",
        "chunk_texts = dialogue_chunker(all_turns)\n",
        "\n",
        "print(f\"‚úÖ Created {len(chunk_texts)} chunks.\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# === CONVERT TO DOCUMENT OBJECTS ===\n",
        "# The Document function by langchain loops through every chunk in chunk_texts and wraps it in a Document object\n",
        "documents = [Document(page_content=chunk) for chunk in chunk_texts]\n",
        "uuids = [str(uuid4()) for _ in documents] # Creates a list of unique string IDs, one for each document\n",
        "\n",
        "# === SETUP VECTOR DATABASE ===\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"example_collection\",\n",
        "    embedding_function=embedder,\n",
        "    persist_directory=CHROMA_PATH,\n",
        ")\n",
        "\n",
        "# === ADD DOCUMENTS TO VECTOR STORE ===\n",
        "print(\"\\nüì¶ Inserting ALL documents (this may take a while)...\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "# Loop through each document and, in batches of 1000 documents, store each document as embedded/vectorized instances in the vector store.\n",
        "for i in range(0, len(documents), BATCH_SIZE):\n",
        "    batch = documents[i:i + BATCH_SIZE]\n",
        "    batch_ids = uuids[i:i + BATCH_SIZE]\n",
        "    print(f\"\\n‚û°Ô∏è [Batch {i} ‚Äì {i + len(batch)}] Generating embeddings...\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    try:\n",
        "        contents = [doc.page_content for doc in batch] #loop through every document in batch\n",
        "        embeds = embedder.embed_documents(contents) # embed the content\n",
        "        print(f\"‚úÖ [Batch {i}] Embeddings done. Now inserting...\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Insert docs one-by-one (This will show us if anything crashes)\n",
        "        for j in range(len(batch)):\n",
        "            try:\n",
        "                vector_store.add_documents([batch[j]], ids=[batch_ids[j]])\n",
        "                print(f\"   ‚§∑ ‚úÖ Inserted doc {i + j}\")\n",
        "                sys.stdout.flush()\n",
        "            except Exception as doc_error:\n",
        "                print(f\"   ‚§∑ ‚ùå Failed to insert doc {i + j}: {doc_error}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ [Batch {i}] Batch complete.\")\n",
        "        gc.collect() # This forces a garbage collection pass ‚Äî it goes through all objects in memory and frees up anything that‚Äôs no longer being used, right now. This is used for memory intensive tasks.\n",
        "        time.sleep(SLEEP_SECONDS)\n",
        "\n",
        "    except Exception as embed_error:\n",
        "        print(f\"‚ùå [Batch {i}] Embedding or Insert Failed:\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.stdout.flush()\n",
        "        continue\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUIsMWi39A5k"
      },
      "outputs": [],
      "source": [
        "# Finally, import the database to local machine so it can be used repeatedly for acting as retrieval source for chatbot\n",
        "shutil.make_archive(\"my_folder_zip\", 'zip', \"ChromaDB\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
